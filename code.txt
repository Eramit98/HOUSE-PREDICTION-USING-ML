import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset

data = {
    'area': [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500],
    'bedrooms': [2, 3, 3, 4, 4, 4, 5, 5, 6, 6],
    'bathrooms': [1, 2, 2, 3, 3, 3, 4, 4, 5, 5],
    'price': [50, 70, 85, 110, 125, 130, 150, 165, 180, 200]
}
df = pd.DataFrame(data)


print("Dataset:\n", df)

# Separate features (X) and target variable (y)
X = df[['area', 'bedrooms', 'bathrooms']]  # Features
y = df['price']  # Target (house price)

# Split the data into training and testing sets,20% use for testing and 80% use for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Linear Regression model
model = LinearRegression()

# Train the model using the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Print the model's coefficients ,tells impact of each feature to predict the price
print("Model Coefficients:", model.coef_)

# Print the intercept,if value of every feature is zero then what will be the predicted price
print("Intercept:", model.intercept_)

# Evaluate the model,less mse means model work good 
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Visualize the actual vs predicted prices
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='green')  # Line for perfect prediction
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted House Prices")
plt.show()